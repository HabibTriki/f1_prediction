version: '3.8'

services:
  # Existing Zookeeper service
  zookeeper:
    image: bitnami/zookeeper:3.8
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    volumes:
      - "zookeeper_data:/bitnami/zookeeper"

  # Existing Kafka service
  kafka:
    image: bitnami/kafka:3.4
    container_name: kafka
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_HOST://:29092
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
    depends_on:
      - zookeeper
    volumes:
      - "kafka_data:/bitnami/kafka"

  # PostgreSQL Database
  postgres:
    image: bitnami/postgresql:15
    container_name: postgres
    environment:
      - POSTGRESQL_USERNAME=f1user
      - POSTGRESQL_PASSWORD=f1password
      - POSTGRESQL_DATABASE=formula1
      - POSTGRESQL_POSTGRES_PASSWORD=adminpassword
    ports:
      - "5432:5432"
    volumes:
      - "postgres_data:/bitnami/postgresql"
      - "./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql"

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U f1user -d formula1"]
      interval: 5s
      timeout: 5s
      retries: 5

  # HDFS Cluster
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=hdfs-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_replication=1
      - HDFS_DATANODE_HOSTNAME=localhost

    ports:
      - "9000:9000"
      - "50070:50070"
    volumes:
      - namenode_data:/hadoop/dfs/namenode

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: datanode
    hostname: datanode  
    environment:
      - CLUSTER_NAME=hdfs-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_datanode_address=datanode:50010      
      - HDFS_CONF_dfs_datanode_http_address=datanode:50075 
      - HDFS_CONF_dfs_datanode_ipc_address=datanode:50020  
      - HDFS_CONF_dfs_replication=1
    depends_on:
      - namenode
    ports:
      - "50075:50075"  # Add this port mapping
      - "9864:9864"
    volumes:
      - datanode_data:/hadoop/dfs/datanode

  # Spark Cluster
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - HOME=/tmp
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - kafka
      - postgres
    volumes:
      - ./processing:/opt/bitnami/spark/app
    ports: []

  spark-worker:
    image: bitnami/spark:3.5
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master
    expose: []

volumes:
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local
  postgres_data:
    driver: local
  namenode_data:
    driver: local
  datanode_data:
    driver: local